[default]
# Batch size during training
batch_size = 35
# Size for hidden dimension
hidden_dim = 128
# Number of layers in LSTM
num_layers = 2
sigma_prior = 1.0
sigma_init_low=50
sigma_init_high=10
# do you want to clip the norm of gradients during training? Make larger than zero to enable
clip_norm = 10.0
# Which optimizer to use. Use 'sgd' or 'adam'
optimizer_name = adam
# learning rate
learning_rate = 0.0025
# Maximum number of training steps
max_steps = 50000
# Size of the convolutional filter
filter_size = 6
# Number of filters per convolutional layer
num_filters=90,90,90,90

# Method string for which experiments to perform
# <METHOD NAME>,<METHOD VARIABLE NAME>,<START VALUE>,<STOP VALUE>|
# For defining a new experiment, a function in util/mutilation.py must have the method name
experiments = rotation,angle,0,90|noise,sigma,0,2
# |noise_clip,sigma,0,1

[sampling]
# Number of interpolations between START_VALUE and STOP_VALUE in an experiment
num_experiments = 35
# For dropout, this is the number of runs with different masks.
# For Bootstrap and Langevin, this is the number of parameter samples
# Please smile if you know why this number is twelve :)
num_runs = 12
# Batch size during testing
batch_size_test = 64

[direc]
data_direc = data/mnist
log_direc = log
restore_direc = log/mnist_final_augment/save/my-model
input_direc = input

