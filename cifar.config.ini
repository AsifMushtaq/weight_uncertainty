[default]
# Batch size during training
batch_size = 35
# Size for hidden dimension
hidden_dim = 128
# Number of layers in LSTM
num_layers = 2
sigma_prior = 1.0
sigma_init_low=100
sigma_init_high=50
# do you want to clip the norm of gradients during training? Make larger than zero to enable
clip_norm = -10.0
# Which optimizer to use. Use 'sgd' or 'adam'
optimizer_name = adam
# learning rate
learning_rate = 0.001
# Maximum number of training steps
max_steps = 100000
# Size of the convolutional filter
filter_size = 6
# Number of filters per convolutional layer
num_filters=200,200,200
# Stride for each filter
stride = 2

# Method string for which experiments to perform
# <METHOD NAME>,<METHOD VARIABLE NAME>,<START VALUE>,<STOP VALUE>|
# For defining a new experiment, a function in util/mutilation.py must have the method name
experiments = rotation,angle,0,70|noise,sigma,0,1.5|warp,warpvalue,0,0.3
# |noise_clip,sigma,0,1

[sampling]
# Number of interpolations between START_VALUE and STOP_VALUE in an experiment
num_experiments = 50
# For dropout, this is the number of runs with different masks.
# For Bootstrap and Langevin, this is the number of parameter samples
# Please smile if you know why this number is twelve :)
num_runs = 12
# Batch size during testing
batch_size_test = 64


[direc]
data_direc = data/cifar
log_direc = log
restore_direc = log/cifar_final/save/my-model
input_direc = input

